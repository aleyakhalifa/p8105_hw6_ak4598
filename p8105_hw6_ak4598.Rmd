---
title: "p8105_hw6_ak4598"
author: "Aleya Khalifa"
date: '`r Sys.Date()`'
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)

set.seed(1)
```

# Problem 2

```{r import_2, include = FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

## 2.1 Set up regression

```{r}
fit2 = lm(tmax ~ tmin + prcp, data = weather_df)
```

Create desired output *without* uncertainty analysis

**r^2^**

```{r}
fit2 %>% 
  broom::glance() %>%
  select(r.squared)
```

log(β1*β1)

```{r}
beta1 = fit2 %>% 
  broom::tidy() %>%
  filter(term == "tmin") %>%
  select(estimate) %>%
  as.numeric()

beta2 = fit2 %>% 
  broom::tidy() %>%
  filter(term == "prcp") %>%
  select(estimate) %>%
  as.numeric()

log(abs(beta1) * abs(beta2))
```


## 2.2 Bootstrap

First, create bootstrap function

```{r}
boot_sample = function(df) {
  sample_frac(df, replace = TRUE)
}
```

Then, draw 5,000 bootstrap samples.

```{r}
boot_straps = 
  tibble(strap_number = 1:5000) %>% 
  mutate(
    strap_sample = map(strap_number, \(i) boot_sample(df = weather_df))
  )

boot_straps
```

Run above on the simulated 5,000 samples. For each bootstrap sample, I produce estimates the two estimates above. I then summarize the 2.5 and 97.5 quantiles to show the 95% confidence interval.

**r^2^**

```{r}
bootstrap_results = 
  boot_straps %>% 
  mutate(
    models = map(strap_sample, \(df) lm(tmax ~ tmin + prcp, data = df) ),
    results = map(models, broom::glance)) %>% 
  select(-strap_sample, -models) %>% 
  unnest(results) 

bootstrap_results %>% 
  summarize(lower_r2 = quantile(r.squared, .025),
            upper_r2 = quantile(r.squared, .975)) %>% 
  knitr::kable(digits = 3)
```

The table above shows the 95% confidence interval for the r^2^ term.

log(β1\*β1)

*Note: the log() function returns NaN if the value is negative, so here I use the absolute value of `tmin`*

```{r}
bootstrap_results = 
  boot_straps %>% 
  mutate(
    models = map(strap_sample, \(df) lm(tmax ~ tmin + prcp, data = df) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap_sample, -models) %>% 
  unnest(results) 

bootstrap_results %>% 
  filter(term == "tmin" | term == "prcp") %>%
  select(strap_number, term, estimate) %>%
  pivot_wider(names_from = term, values_from = estimate) %>%
  mutate(log_betas = log(abs(tmin) * abs(prcp))) %>%
  summarize(lower = quantile(log_betas, .025, na.rm = TRUE),
         upper = quantile(log_betas, .975)) %>%
  knitr::kable(digits = 3)
```

The table above shows the 95% confidence interval for the log(β1\*β1) term.

# Problem 3

```{r import_3, include = FALSE}
birthweight = read_csv("data/birthweight.csv") %>%
  janitor::clean_names()
```

## 3.1 Regression

Fit regression model that predicts baby's birth weight in `bwt`. I posit that the family' monthly income `fincome`, its gestational age in weeks `gaweeks`, mother's race `mrace`, and mother' pre-pregnancy BMI `ppbmi` predict baby's birth weight. First, I have to check the data quality in these variables and clean. All are continuous except for `mrace`, so I format that here. White will be the reference category.

```{r}
birthweight = birthweight %>%
  mutate(mrace = factor(mrace,
                        levels = c(1,2,3,4),
                        labels = c("White",
                                      "Black",
                                      "Asian",
                                      "Puerto Rican")))
```

Check for missing data - none!

```{r}
sapply(birthweight, function(x) sum(is.na(x))/nrow(birthweight))
```

Now I can fit the regression model.

```{r}
fit3a = lm(bwt ~ fincome + gaweeks + mrace + ppbmi, 
          data = birthweight)
```

## 3.2 Model diagnostics

Let's add residuals to a data frame.

```{r}
residuals = modelr::add_residuals(birthweight, fit3a) %>%
  select(resid) %>%
  mutate(resid = as.numeric(resid)) %>%
  rowid_to_column("id")
```

And fitted values:

```{r}
fitted = modelr::add_predictions(birthweight, fit3a) %>% 
  select(pred) %>%
  mutate(pred = as.numeric(pred)) %>%
  rowid_to_column("id")
```

Here is a plot of model residuals against fitted values.

```{r plot_3.1}
diagnostics = full_join(residuals, fitted, by = "id")


ggplot(diagnostics, aes(x = pred, y = resid)) + 
  geom_point(size = 2, alpha = 0.8, shape = 21, color = "blue") + 
  labs(title = "Model fitted values vs. residuals",
       x = "Fitted value",
       y = "Residual")
```

Since the points are around the 0-line for residuals, the relationship seems to be linear. But since the mean residual doesn't change with the fitted values, the spread of residuals is not constant. There don't seem to be any stand-out outliers.

## 3.3 Cross-validation

Now let's compare this model to two others.

`fit3b`: length at birth and gestational age as predictors (main effects only)

```{r}
fit3b = lm(bwt ~ blength + gaweeks, data = birthweight)
```

and `fit3c`: head circumference, length, sex, and all interactions (including the three-way interaction) between these

```{r}
fit3c = lm(bwt ~ bhead + blength + babysex + 
             bhead*babysex + blength*babysex + bhead*blength, 
           data = birthweight)
```

We can compare the three models using cross-validated prediction error.

```{r}
cv_df = 
  crossv_mc(birthweight, 100) %>%
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)) %>%
  mutate(
    fit3a = map(train, \(df) fit3a),
    fit3b  = map(train, \(df) fit3b),
    fit3c  = map(train, \(df) fit3c)) %>%
  mutate(
    rmse_a = map2_dbl(fit3a, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_b = map2_dbl(fit3b, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_c = map2_dbl(fit3c, test, \(mod, df) rmse(model = mod, data = df)))
```

We can compare the RMSEs of each visually, which gives us a picture of prediction error.

```{r plot_3.2}
cv_df %>%
  select(starts_with("rmse")) %>%
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>%
  mutate(model = fct_inorder(model)) %>%
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin() + 
  labs(title = "3-Model Comparison",
       x = "Model",
       y = "Root Mean Squared Errors (RMSE)")
```

According to this plot, Model C seems to be the best at predicting baby's birthweight.


